{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resumption of the session', 'hervatting van de zitting'], ['i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period', 'ik verklaar de zitting van het europees parlement die op vrijdag december werd onderbroken te zijn hervat ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad'], ['although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful', 'zoals u heeft kunnen constateren is de grote millenniumbug uitgebleven de burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen'], ['you have requested a debate on this subject in the course of the next few days during this partsession', 'u heeft aangegeven dat u deze vergaderperiode een debat wilt over deze rampen'], ['in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union', 'nu wil ik graag op verzoek van een aantal collegas een minuut stilte in acht nemen ter nagedachtenis van de slachtoffers ik doel hiermee met name op de slachtoffers van het noodweer dat verschillende lidstaten van de unie heeft geteisterd']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load doc into memory\n",
    "with open(\"cleaned_pairs.txt\", 'rb') as f:\n",
    "\tpaired_sent = pickle.load(f)\n",
    "\n",
    "print(paired_sent[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeVocab():\n",
    "    def __init__(self) -> None:\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                # First entry of word into vocabulary\n",
    "                self.word2index[word] = self.num_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.num_words] = word\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # Word exists; increase word count\n",
    "                self.word2count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "            sentence_len = 0\n",
    "            for word in sentence.split(' '):\n",
    "                sentence_len += 1\n",
    "                self. add_word(word)\n",
    "            if sentence_len > self.longest_sentence:\n",
    "                # This is the longest sentence\n",
    "                self.longest_sentence = sentence_len\n",
    "            # Count the number of sentences\n",
    "            self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_eng = MakeVocab()\n",
    "vocab_nl = MakeVocab()\n",
    "for pair in paired_sent:\n",
    "    vocab_eng.add_sentence(pair[0])\n",
    "    vocab_nl.add_sentence(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resumption\n",
      "hervatting\n"
     ]
    }
   ],
   "source": [
    "print(vocab_eng.to_word(3))\n",
    "print(vocab_nl.to_word(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embeddings = nn.Embedding(num_words, 10)\n",
    "lookup_tensor = torch.tensor([word2index[\"zitting\"]])\n",
    "zitting_embed = embeddings(lookup_tensor)\n",
    "print(zitting_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method Two for embedding \n",
    "\n",
    "#create vocabulary\n",
    "def vocab(sentences):\n",
    "    vocabulary = set()\n",
    "    vocabulary.update(['<sos>', '<eos>', '<pad>'])\n",
    "    for sen in sentences:\n",
    "        vocabulary.update(sen.split())\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "    return (word_to_index,index_to_word)\n",
    "#change the words which are in the sentences into indices\n",
    "def sentence_to_index(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab[word] for word in sentence.split()] + [vocab['<eos>']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1997775, 1997775)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# load doc into memory\n",
    "with open(\"sen_eng.txt\", 'rb') as f:\n",
    "\tsentences_eng = pickle.load(f)\n",
    "\t\n",
    "with open(\"sen_nl.txt\", 'rb') as f:\n",
    "\tsentences_nl = pickle.load(f)\n",
    "\t\n",
    "nl_vocab,nl_index_to_word = vocab(sentences_nl)\n",
    "en_vocab,en_index_to_word = vocab(sentences_eng)\n",
    "nl_index = [torch.Tensor(sentence_to_index(sen, nl_vocab)) for sen in sentences_nl]\n",
    "en_index = [torch.Tensor(sentence_to_index(sen, en_vocab)) for sen in sentences_eng]\n",
    "\n",
    "len(nl_index), len(en_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset for translation\n",
    "#use Dataloader to get the dataloader for training dataset, validation dataset and test dateset: 49% : 21% : 30%\n",
    "\n",
    "class Builddataset(Dataset):\n",
    "    def __init__(self, sentences_eng, sentences_nl):\n",
    "        self.nl_sentences = sentences_nl\n",
    "        self.english_sentences = sentences_eng\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.nl_sentences[idx], self.english_sentences[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    en_index, nl_index = zip(*batch)\n",
    "\n",
    "    padded_english_sentences = pad_sequence(en_index, batch_first=True, padding_value=en_vocab['<pad>'])\n",
    "    padded_nl_sentences = pad_sequence(nl_index, batch_first=True, padding_value=nl_vocab['<pad>'])\n",
    "    \n",
    "    return padded_nl_sentences, padded_english_sentences, \n",
    "\n",
    "\n",
    "\n",
    "dataset = Builddataset(nl_index, en_index)\n",
    "\n",
    "\n",
    "train_size = int(0.49 * len(dataset))\n",
    "val_size = int(0.21* len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=72, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=72, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=72, shuffle=False, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
