{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resumption of the session', 'hervatting van de zitting'], ['i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period', 'ik verklaar de zitting van het europees parlement die op vrijdag december werd onderbroken te zijn hervat ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad'], ['although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful', 'zoals u heeft kunnen constateren is de grote millenniumbug uitgebleven de burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen'], ['you have requested a debate on this subject in the course of the next few days during this partsession', 'u heeft aangegeven dat u deze vergaderperiode een debat wilt over deze rampen'], ['in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union', 'nu wil ik graag op verzoek van een aantal collegas een minuut stilte in acht nemen ter nagedachtenis van de slachtoffers ik doel hiermee met name op de slachtoffers van het noodweer dat verschillende lidstaten van de unie heeft geteisterd']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load doc into memory\n",
    "with open(\"cleaned_pairs.txt\", 'rb') as f:\n",
    "\tpaired_sent = pickle.load(f)\n",
    "\n",
    "print(paired_sent[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeVocab():\n",
    "    def __init__(self) -> None:\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                # First entry of word into vocabulary\n",
    "                self.word2index[word] = self.num_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.num_words] = word\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # Word exists; increase word count\n",
    "                self.word2count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "            sentence_len = 0\n",
    "            for word in sentence.split(' '):\n",
    "                sentence_len += 1\n",
    "                self. add_word(word)\n",
    "            if sentence_len > self.longest_sentence:\n",
    "                # This is the longest sentence\n",
    "                self.longest_sentence = sentence_len\n",
    "            # Count the number of sentences\n",
    "            self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_eng = MakeVocab()\n",
    "vocab_nl = MakeVocab()\n",
    "for pair in paired_sent:\n",
    "    vocab_eng.add_sentence(pair[0])\n",
    "    vocab_nl.add_sentence(pair[1])\n",
    "if vocab_eng.longest_sentence > vocab_nl.longest_sentence:\n",
    "    longest_sentence = vocab_eng.longest_sentence\n",
    "else:\n",
    "    longest_sentence = vocab_nl.longest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resumption\n",
      "hervatting\n"
     ]
    }
   ],
   "source": [
    "print(vocab_eng.to_word(3))\n",
    "print(vocab_nl.to_word(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "EOS_token = 1\n",
    "SOS_token = 0\n",
    "\n",
    "def indexesFromSentence(word2index, sentence):\n",
    "    return [word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "\n",
    "    n = len(paired_sent)\n",
    "    input_ids = np.zeros((n, longest_sentence + 1), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, longest_sentence + 1), dtype=np.int32)\n",
    "    \n",
    "    for index, (input, target) in enumerate(paired_sent):\n",
    "        inp_ids = indexesFromSentence(vocab_eng.word2index, input)\n",
    "        tgt_ids = indexesFromSentence(vocab_nl.word2index, target)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[index, :len(inp_ids)] = inp_ids\n",
    "        target_ids[index, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    print(input_ids[:5])\n",
    "    print(target_ids[:5])\n",
    "    '''\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids),\n",
    "                               torch.LongTensor(target_ids))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "    '''\n",
    "\n",
    "get_dataloader(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
