{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load English data\n",
    "filename_eng = 'europarl-v7.nl-en.en'\n",
    "eng_data = load_doc(filename_eng)\n",
    "\n",
    "# load Dutch data\n",
    "filename_nl = 'europarl-v7.nl-en.nl'\n",
    "nl_data = load_doc(filename_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and loading of data using:\n",
    "https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resumption of the session\n",
      "i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
      "although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
      "hervatting van de zitting\n",
      "ik verklaar de zitting van het europees parlement die op vrijdag december werd onderbroken te zijn hervat ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad\n",
      "zoals u heeft kunnen constateren is de grote millenniumbug uitgebleven de burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "\treturn doc.strip().split('\\n')\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# normalize unicode characters\n",
    "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\tline = line.decode('UTF-8')\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [word.translate(table) for word in line]\n",
    "\t\t# remove non-printable chars form each token\n",
    "\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\treturn cleaned\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "\n",
    "# transform data into sentences and clean them\n",
    "sentences_eng = to_sentences(eng_data)\n",
    "sentences_eng = clean_lines(sentences_eng)\n",
    "# spot check\n",
    "for i in range(3):\n",
    "\tprint(sentences_eng[i])\n",
    "\n",
    "\n",
    "# transform data into sentences and clean them\n",
    "sentences_nl = to_sentences(nl_data)\n",
    "sentences_nl = clean_lines(sentences_nl)\n",
    "# spot check\n",
    "for i in range(3):\n",
    "\tprint(sentences_nl[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index itself in the dictionary is the thing that is embedded.\n",
    "The word is the key and the index is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ik verklaar de zitting van het europees parlement die op vrijdag december werd onderbroken te zijn hervat ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_nl[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html?__cf_chl_tk=LPyQpuVXIlt8Tfy._P5aqelhRrIBj14.w1y7j3sgD0c-1710433843-0.0.1.1-1685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "word2count = {}\n",
    "index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "num_words = 3\n",
    "num_sentences = 0\n",
    "longest_sentence = 0\n",
    "\n",
    "def add_word(word):\n",
    "        if word not in word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            global num_words\n",
    "            word2index[word] = num_words\n",
    "            word2count[word] = 1\n",
    "            index2word[num_words] = word\n",
    "            num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            word2count[word] += 1\n",
    "\n",
    "def add_sentence(sentence):\n",
    "        global num_sentences\n",
    "        global longest_sentence\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split(' '):\n",
    "            sentence_len += 1\n",
    "            add_word(word)\n",
    "        if sentence_len > longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        num_sentences += 1\n",
    "\n",
    "def to_word(index):\n",
    "        return index2word[index]\n",
    "\n",
    "def to_index(word):\n",
    "    return word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "\n",
    "for sentence in sentences_nl:\n",
    "    add_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hervatting\n"
     ]
    }
   ],
   "source": [
    "word = to_word(3)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html?__cf_chl_tk=LPyQpuVXIlt8Tfy._P5aqelhRrIBj14.w1y7j3sgD0c-1710433843-0.0.1.1-1685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2887, -0.2696, -1.3066,  0.2335, -0.9795,  0.4110, -0.7521, -1.3908,\n",
      "          1.0985, -0.2476]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.1829,  0.6211,  1.8577, -0.1978, -1.3098,  0.7570, -0.8599,  0.0913,\n",
      "         -0.8472,  1.1889]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embeddings = nn.Embedding(num_words, 10)\n",
    "lookup_tensor = torch.tensor([word2index[\"zitting\"]])\n",
    "zitting_embed = embeddings(lookup_tensor)\n",
    "print(zitting_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
