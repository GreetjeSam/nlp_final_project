{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load English data\n",
    "filename_eng = 'europarl-v7.nl-en.en'\n",
    "eng_data = load_doc(filename_eng)\n",
    "\n",
    "# load Dutch data\n",
    "filename_nl = 'europarl-v7.nl-en.nl'\n",
    "nl_data = load_doc(filename_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and loading of data using:\n",
    "https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_eng.txt\n",
      "resumption of the session\n",
      "i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
      "although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
      "Saved: cleaned_nl.txt\n",
      "hervatting van de zitting\n",
      "ik verklaar de zitting van het europees parlement die op vrijdag december werd onderbroken te zijn hervat ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad\n",
      "zoals u heeft kunnen constateren is de grote millenniumbug uitgebleven de burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "\treturn doc.strip().split('\\n')\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# normalize unicode characters\n",
    "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\tline = line.decode('UTF-8')\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [word.translate(table) for word in line]\n",
    "\t\t# remove non-printable chars form each token\n",
    "\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\treturn cleaned\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "\tfile = open(filename, 'w', encoding='utf-8-sig')\n",
    "\tfor sentence in sentences:\n",
    "\t\tfile.write(sentence+\"\\n\")\n",
    "\tfile.close()\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "\n",
    "# transform data into sentences and clean them\n",
    "sentences_eng = to_sentences(eng_data)\n",
    "sentences_eng = clean_lines(sentences_eng)\n",
    "\n",
    "save_clean_sentences(sentences_eng, \"cleaned_eng.txt\")\n",
    "\n",
    "# spot check\n",
    "for i in range(3):\n",
    "\tprint(sentences_eng[i])\n",
    "\n",
    "\n",
    "# transform data into sentences and clean them\n",
    "sentences_nl = to_sentences(nl_data)\n",
    "sentences_nl = clean_lines(sentences_nl)\n",
    "\n",
    "save_clean_sentences(sentences_nl, \"cleaned_nl.txt\")\n",
    "\n",
    "# spot check\n",
    "for i in range(3):\n",
    "\tprint(sentences_nl[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
